# -*- coding: utf-8 -*-
"""ML_3_41157.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eVwV7CZv7cIOCdmogM6kb_BkZp3xs3RF

# Given a bank customer, build a neural network-based classifier that can determine whether they will leave or not in the next 6 months.
Dataset Description: The case study is from an open-source dataset from Kaggle. The dataset contains 10,000 sample points with 14 distinct features such as CustomerId, CreditScore, Geography, Gender, Age, Tenure, Balance, etc.
Link to the Kaggle project: https://www.kaggle.com/barelydedicated/bank-customer-churn-modeling Perform following steps:
1.	Read the dataset.
2.	Distinguish the feature and target set and divide the data set into training and test sets.
3.	Normalize the train and test data.
4.	Initialize and build the model. Identify the points of improvement and implement the same.
5.	Print the accuracy score and confusion matrix.
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt #Importing the libraries

df = pd.read_csv("/content/Churn_Modelling.csv")

"""# Preprocessing."""

df.head()

df.shape

df.describe()

df.isnull()

df.isnull().sum()

df.info()

df.dtypes

df.columns  # our target column is exited 1 mean customer left bank 0 mean customer exist in bank

df.duplicated().sum()

df['Exited'].value_counts()    # 0 indicate people stay with bank and 1 left the bank shown in balance

df['Geography'].value_counts()

df['Gender'].value_counts()

df.drop(columns=['RowNumber','CustomerId','Surname'],inplace=True)  #dropping unnecssary column inplace means permanantly removed

df.head()

# Now convert categorical column into one hot encoder Geography and gender

df=pd.get_dummies(df,columns=['Geography','Gender'],drop_first=True)   #drop first it helps in reducing the extra column created during dummy variable creation. see gender

df

# Now see column X and Y
X = df.drop(columns=['Exited'])
y = df['Exited']

X

y

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3, random_state=1)

X_train.shape

# Now scale the values

# Normalizing the values with mean as 0 and Standard Deviation as 1

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

X_train_scaled = scaler.fit_transform(X_train)

X_test_scaled = scaler.transform(X_test)

X_train_scaled   # shown 2 D array with small values

X_train_scaled.shape

X_test_scaled

"""# Visualization"""

def visualization(x, y, xlabel):
    plt.figure(figsize=(10,5))
    plt.hist([x, y], color=['red', 'green'], label = ['exit', 'not_exit'])
    plt.xlabel(xlabel,fontsize=20)
    plt.ylabel("No. of customers", fontsize=20)
    plt.legend()

df_churn_exited = df[df['Exited']==1]['Tenure']     #customer left the bank
df_churn_not_exited = df[df['Exited']==0]['Tenure']  #customer not left the bank

visualization(df_churn_exited, df_churn_not_exited, "Tenure")

df_churn_exited2 = df[df['Exited']==1]['Age']
df_churn_not_exited2 = df[df['Exited']==0]['Age']

visualization(df_churn_exited2, df_churn_not_exited2, "Age")

"""# Building the Classifier Model using Keras"""

import keras #Keras is an Open Source Neural Network library written in Python that runs on top of Theano or Tensorflow.
# we Can use Tenserflow as well but won't be able to understand the errors initially.

from keras.models import Sequential #To create sequential neural network layers in a sequential order
from keras.layers import Dense #To create hidden layers

classifier = Sequential()  # sequential is class name ie a predictive modeling problem where you have some sequence of inputs over space or time, and the task is to predict a category for the sequence
#Units: it denotes the output size of the layer, normally average of no of node in input layer (no of independent variable) which is 11 and no of node in output layer which is 1, we took 6 as average.
#Kernel_initializer : The initializer parameters tell Keras how to initialize the values of our layer, weight matrix and our bias vector
#Activation: Element-wise activation function to be used in the dense layer. read more about Rectified Linear Unit (ReLU)
#Input_dim: for first layer only, number of input independent variable. only for first hidden layer
#Bias : if we are going with advance implementation

classifier.add(Dense(units =3 , activation='sigmoid', kernel_initializer='uniform', input_dim = 11))   #input layer 11 hidden layer=3 #uniform is type of distribution

classifier.add(Dense(units =1 , activation='sigmoid', kernel_initializer='uniform',))  # output layer is 1

classifier.summary()

# 11*3 + 3 bias=36
# 3*1 + 1 bias = 4
# total= 36+4= 40

#!pip install ann_visualizer
#!pip install graphviz

#from ann_visualizer.visualize import ann_viz;
#Build your model here
#ann_viz(classifier,title="ANN Model Diagram");

# Now compile model using loss function it is binary classification problem


# Optimizer: update the weight parameters to minimize the loss function..
# Loss function: acts as guides to the terrain telling optimizer if it is moving in the right direction to reach the bottom of the valley, the global minimum.
# Metrics: A metric function is similar to a loss except that the results from evaluating a metric are not used when training the model.
# Batch size: hyper-parameter related to sample
# Epochs: hyper-parameter related to iteration

classifier.compile(optimizer="adam",loss = 'binary_crossentropy',metrics = ['accuracy']) #To compile the Artificial Neural Network. Ussed Binary crossentropy as we just have only two output
classifier.fit(X_train_scaled,y_train,batch_size = 10,epochs=10, validation_split=0.2 )

# now check the value of weight and bias value

classifier.layers[0].get_weights()

# output shown 33 layers connection 3 bias

classifier.layers[1].get_weights()

# output shown 3 layers connection 1 bias

# Now predict the model

classifier.predict(X_test_scaled)

# output not shown 1 or 0 because you use sigmoid due to this we need convert the probability into 0 and 1

# assume threshold 0.5
# if threshold less than 0.5 customer left the bank
# if threshold greater than 0.5 customer not left the bank

y_log= classifier.predict(X_test_scaled)  # y_log is just name of varriable

y_pred= np.where(y_log>0.5,1,0)

# Now check accuracy of model

from sklearn.metrics import accuracy_score
accuracy_score(y_test,y_pred)

from keras.metrics import Accuracy
# now check accuracy
classifier.compile(loss='binary_crossentropy',optimizer = 'Adam', metrics=['Accuracy'])  # Adam perform good for our gradient decent algorithm

history = classifier.fit(X_train_scaled,y_train,batch_size = 10,epochs=50, validation_split=0.2 )   # validation_split=0.2 mean seperate 20% customer out of avalible 10,000 customer

# output shown loss on training data with accuracy and validation loss and accuracy for 20% testing data ie 0.2 we taken earlier

classifier.layers[0].get_weights()

classifier.layers[1].get_weights()

#classifier.layers[2].get_weights()

y_log= classifier.predict(X_test_scaled)  # y_log is just name of varriable

y_pred= np.where(y_log>0.5,1,0)

# Now check accuracy of model

from sklearn.metrics import accuracy_score
accuracy_score(y_test,y_pred)

# accuracy shown reduce due to overfitting of model but we need more accuracy

import matplotlib.pyplot as plt

acc=history.history['loss']
val_acc=history.history['Accuracy']
loss=history.history['val_loss']
val_loss=history.history['val_Accuracy']
# so history dictionary created
# out shown training loss , training Accuracy, validation loss, validation accuracy

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])

plt.plot(history.history['Accuracy'])

plt.plot(history.history['val_Accuracy'])

from sklearn.metrics import confusion_matrix,accuracy_score,classification_report

cm = confusion_matrix(y_test,y_pred)

cm

accuracy = accuracy_score(y_test,y_pred)

accuracy

plt.figure(figsize = (10,7))
sns.heatmap(cm,annot = True)
plt.xlabel('Predicted')
plt.ylabel('Truth')

print(classification_report(y_test,y_pred))

#Precision of the model is 83 %. It looks good on paper but we should easily be able to get 100% with a more complex model.